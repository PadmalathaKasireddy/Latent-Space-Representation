# ğŸ” Latent Space Representation using Autoencoders (AE) & Variational Autoencoders (VAE)

This project explores how Autoencoders (AE) and Variational Autoencoders (VAE) can be used to learn compressed, meaningful representations of high-dimensional data such as handwritten digits from the MNIST dataset. It includes training both models, visualizing their latent spaces, evaluating reconstruction quality, and performing latent vector arithmetic.

---

## ğŸ“ Contents

- ğŸ“Œ [Objective](#objective)
- ğŸ§  [Model Architectures](#model-architectures)
- ğŸ“‰ [Training and Loss Comparison](#training-and-loss-comparison)
- ğŸ“Š [Latent Space Visualization](#latent-space-visualization)
- ğŸ§ª [Latent Vector Arithmetic](#latent-vector-arithmetic)
- ğŸ“ [Key Learnings](#key-learnings)
- ğŸš€ [Future Work](#future-work)
- ğŸ“ [Presentation](#presentation)
- ğŸ‘¤ [Author](#author)

---

## ğŸ¯ Objective

1. **Build AE and VAE** using PyTorch/Keras and train on MNIST dataset.
2. **Visualize the Latent Space** to see how images are distributed after compression.
3. **Compare Reconstruction Quality** between AE and VAE.
4. **Perform Latent Vector Arithmetic** to demonstrate interpolation and generative capabilities.

---

## ğŸ§  Model Architectures

### ğŸ”¹ Autoencoder (AE)
- **Encoder**: Compresses input image to a lower-dimensional representation.
- **Decoder**: Reconstructs the image from the encoded vector.
- **Loss**: Mean Squared Error (MSE)
- **Activation Functions**: ReLU in hidden layers, Sigmoid in output layer

### ğŸ”¹ Variational Autoencoder (VAE)
- **Encoder Outputs**: Mean (Î¼) and Standard Deviation (Ïƒ)
- **Reparameterization Trick**: 
  \[
  z = \mu + \sigma \cdot \epsilon \quad \text{where } \epsilon \sim \mathcal{N}(0, 1)
  \]
- **Loss**: Reconstruction Loss + KL Divergence

---

## ğŸ“‰ Training and Loss Comparison

- Both AE and VAE models show decreasing loss over epochs.
- AE trains faster and gives sharper reconstructions.
- VAE converges slowly but learns smoother latent representations useful for generation.

---

## ğŸ“Š Latent Space Visualization

- **AE**: Latent vectors are often entangled and unstructured.
- **VAE**: Latent vectors exhibit clear clustering of classes.
- **Tools Used**: PCA / t-SNE for 2D projection of high-dimensional latent vectors.

---

## ğŸ§ª Latent Vector Arithmetic

- Performed interpolation between digits, e.g., morphing digit "3" into "8".
- Demonstrates VAE's ability to generate meaningful transitions across latent space.

---

## ğŸ“ Key Learnings

- Latent space acts as a compressed representation where similar data points are close.
- AE is better at compression, VAE is better at generation.
- Visualization tools help understand abstract features learned by the model.

---

## ğŸš€ Future Work

- Train on more complex datasets (e.g., CIFAR-10).
- Experiment with different latent dimensions and loss functions.
- Explore conditional VAEs or GANs for better generative performance.

---

## ğŸ“ Presentation

The entire project is documented in the following presentation:

ğŸ“‚ **[Google Colab Notebook](https://colab.research.google.com/drive/1raVa0_2TtQY7wO0hku1kk--urz-vJAaa?usp=sharing)**  
ğŸ“„ **[PPT File](""C:\Users\PADMALATHA\Latent-Space-Representation-using-AE-and-VAE.pptx"")**

---

## ğŸ‘¤ Author

**Padmalatha Kasireddy**  
Department of CSE (AI & ML), GITAMW  
ğŸ“§ Email: padmalatha3633@gmail.com  
ğŸ”— LinkedIn: *[Add your LinkedIn profile link]*

---

## ğŸ§  Acknowledgements

Special thanks to **NextHub Technologies Pvt Ltd** for providing mentorship as part of the Generative AI Internship program.

---



